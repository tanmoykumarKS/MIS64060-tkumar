---
title: "Machine Learning Assignment 2"
output:
  html_document:
    df_print: paged
    toc: true
  html_notebook:
    highlight: textmate
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
dir.create("images")
```

***

# k-NN

This notebook illustrates the code for Classifiers module. We will continue to use the Caret Package. The caret package (short for Classification And REgression Training) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:

* data splitting/partitioning
* pre-processing
* feature selection
* model tuning using resampling 

***

Some code in this package comes from the following reference:

* https://www.dataminingbook.com/book/r-edition

***

Install packages if necessary. Uncomment before running.

```{r}
#install.packages("caret")
library(caret)

# install.packages("ISLR") # only install if needed
library(ISLR)

library(FNN)

```

***
Loading the data file and reviewing data structures

```{r}
UniversalBank<-read.csv("UniversalBank.csv")
head(UniversalBank)
summary(UniversalBank)
```

Excluding ID and Zip Code and selecting the data set with the mentioned criteria

```{r}
UniversalBank2<-UniversalBank[,c(-1,-5)]
UniversalBank1<-UniversalBank2[which(UniversalBank2$Age ==40 | UniversalBank2$Experience==10 | UniversalBank2$Income==84 | UniversalBank2$Family==2 | UniversalBank2$CCAvg==2 | UniversalBank2$Education==1 | UniversalBank2$Mortgage==0 | UniversalBank2$Securities.Account==0 | UniversalBank2$CD.Account==0 | UniversalBank2$Online==1 | UniversalBank2$CreditCard==1),]
str(UniversalBank1)
```


## Data Splitting

Assume that we are interested in creating a training and validation set from this dataset. To simplify our illustration, we will restric ourself to the following three variables: Sales, Education, Urban, and use only a small dataset for illustration

* First, we create a dataset with only the required columns. We will use the dplyr package. Install it if necessary
* Then, we create an index for the training sample.
* We next create the training dataset
* We then use the reverse index of the training sample to create the validation set


```{r}
library(dplyr)

set.seed(15)
Test_Index = createDataPartition(UniversalBank1$Age,p=0.2, list=FALSE) # 20% reserved for Test
Test_Data = UniversalBank1[Test_Index,]
TraVal_Data = UniversalBank1[-Test_Index,] # Validation and Training data is rest

Train_Index = createDataPartition(TraVal_Data$Age,p=0.60, list=FALSE) # 60% of remaining data as training
Train_Data = TraVal_Data[Train_Index,]
Validation_Data = TraVal_Data[-Train_Index,] # rest as validation

summary(Train_Data)
summary(Validation_Data)
summary(Test_Data)
```

***

## Plotting

Let us plot the data. I recommend you use ggplot, instead of the plot command. 

```{r}
library(ggplot2)
ggplot(Train_Data, aes(x=Age,y=Income, color=Experience)) +
  geom_point() 

```

***

## Normalization

Let us now normalize the data. 

The preProcess ( ) function that is in the ‘caret’ package is a powerful method that has implemented a number of data processing and transformation methods.

 The function implements min-max normalization using “range” as the method or z-score scaling when using “center” and “scale” as input method parameters.
```{r}
# Copy the original data
train.norm.df <- Train_Data
valid.norm.df <- Validation_Data
traval.norm.df <- TraVal_Data

# use preProcess() from the caret package to normalize Sales and Age.
norm.values <- preProcess(Train_Data, method=c("center", "scale"))

train.norm.df <- predict(norm.values, Train_Data) # Replace all the columns with normalized values
valid.norm.df <- predict(norm.values, Validation_Data)
traval.norm.df <- predict(norm.values, traval.norm.df)
test.norm.df <- predict(norm.values, Test_Data)

summary(train.norm.df)
var(train.norm.df)
summary(valid.norm.df)
var(valid.norm.df)
```

***

## Modeling k-NN

Let us now apply knn. knn() is available in library FNN (provides a list of the nearest neighbors), and library class (allows a numerical output variable).

```{r}
#install.packages("FNN") # install if needed
library(FNN)
nn <- knn(train = train.norm.df, test = test.norm.df, 
          cl = train.norm.df[, 8], k = 3, prob=TRUE) # We use k = 3, and Urban is the Y

 #print(nn) # uncomment for more output

#row.names(Train_Data)[attr(nn, "nn.index")]
```

Note the output provides class membership, and also a measure of distances from its nearest neighbors, in this case k=3.

But, how does one choose k?

***

## Hypertuning using Validation

To determine k, we use the performance on the validation set.
Here, we will vary the value of k from 1 to 14

```{r}
# initialize a data frame with two columns: k, and accuracy.
library(caret)
accuracy.df <- data.frame(k = seq(1, 14, 1), accuracy = rep(0, 14))

# compute knn for different k on validation.
for(i in 1:14) {
  knn.pred <- knn(train.norm.df, valid.norm.df, 
                  cl = train.norm.df[, 8], k = i)
  accuracy.df[i, 2] <- confusionMatrix(knn.pred, valid.norm.df[, 8])$overall[1] 
}
accuracy.df
```


The value of k that provides the best performance is k = 9. We apply the results to the test set.

***

## Prediction

Before we predict for the test set, we should combine the Training and Validation set, normalize the data, and then do the prediction. 
```{r}

norm.values <- preProcess(TraVal_Data[, 1:2], method=c("center", "scale")) # Use combined set to normalize

traval.norm.df[, 1:2] <- predict(norm.values, TraVal_Data[, 1:2])
test.norm.df[, 1:2] <- predict(norm.values, Test_Data[, 1:2])
summary(traval.norm.df)
summary(test.norm.df)
```

Now we predict for the test set.

```{r}
knn.pred.new <- knn(traval.norm.df[, 1:2], test.norm.df, 
                    cl = traval.norm.df[, 3], k = 9)
row.names(TraVal_Data)[attr(nn, "nn.index")]
```





