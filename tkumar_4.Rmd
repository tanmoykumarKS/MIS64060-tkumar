---
title: "ML Assignment 4"
output:
  word_document: default
  pdf_document: default
---
## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

Loading the Data
```{r}
rm(list = ls())

library(tidyverse)
#install.packages("factoextra")
library(factoextra)
library(ISLR)
set.seed(123)

DFUniver<-read.csv("Universities.csv")
colnames(DFUniver)
#summary(DFUniver)

#Changing the column names to suitable ones.
DFUniver<-DFUniver%>%rename(
  Pub.Private=Public..1...Private..2.,
  ApplRec=X..appli..rec.d,
  ApplAccept=X..appl..accepted,
  NewStdEnr=X..new.stud..enrolled,
  Top10=X..new.stud..from.top.10.,
  Top25=X..new.stud..from.top.25.,
  FTUnderG=X..FT.undergrad,
  PTUnderG=X..PT.undergrad,
  InStateFee=in.state.tuition,
  OutStateFee=out.of.state.tuition,
  BookCost=estim..book.costs,
  PerCost=estim..personal..,
  PHD=X..fac..w.PHD,
  StFactRatio=stud..fac..ratio
)

colnames(DFUniver)


```
Removing missing records from the Dataset (Measurements)
```{r}
#Total NULL fields in the data frame 
count(DFUniver[!complete.cases(DFUniver),])

#Ipute the NULL values
DFUniver1<-na.omit(DFUniver) 
count(DFUniver1)

```

Finding the Data Summary and Measure of Dependence
```{r}
#Summary Data
summary(DFUniver1)

#Subsetting the data 

DFNumerical<-subset(DFUniver1, select = -c(1,2,3)) 

#Finding the correlation between the data set
library(corrplot)
corrplot(cor(DFNumerical), method = "color")

```
In the correlation graph, Darker Blue(+1) and Dark Orange(-1) shows the higher correlated data. Using this data to understand any correlation among the column data.

Applying K-means clustering for Numeric Data
```{r}
#Scaling the Data
DFNumerical<-scale(DFNumerical)

#Distance Between Observations
distance <- get_dist(DFNumerical)

fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

```

Comparison different cluster values

```{r}

k2 <- kmeans(DFNumerical, centers = 2, nstart = 25)
k3 <- kmeans(DFNumerical, centers = 3, nstart = 25)
k4 <- kmeans(DFNumerical, centers = 4, nstart = 25)
k5 <- kmeans(DFNumerical, centers = 5, nstart = 25)
k6 <- kmeans(DFNumerical, centers = 5, nstart = 25)

# plots to compare
p2 <- fviz_cluster(k2, geom = "point", data = DFNumerical) + ggtitle("k = 2")
p3 <- fviz_cluster(k3, geom = "point",  data = DFNumerical) + ggtitle("k = 3")
p4 <- fviz_cluster(k4, geom = "point",  data = DFNumerical) + ggtitle("k = 4")
p5 <- fviz_cluster(k5, geom = "point",  data = DFNumerical) + ggtitle("k = 5")
p6 <- fviz_cluster(k6, geom = "point",  data = DFNumerical) + ggtitle("k = 6")

library(gridExtra)
grid.arrange(p2, p3, p4, p5, p6, nrow = 2)
```
From the above comparison it seems that 3 clusters would be good. Determining Optimal Cluster using Elbow and Silhouette method. 

```{r}
set.seed(123)
#Finding optimal number of clusters - Elbow Method
fviz_nbclust(DFNumerical, kmeans, method = "wss")

#Determining Optimal Cluster by Average Silhouette Method
fviz_nbclust(DFNumerical, kmeans, method = "silhouette")

#Silhouette method shows that 3 numbers of clusters would be optimum. From previous cluster plotting we have seen that optimal cluster size would be 3. 

#3 clusters are the reasonable for this data and the optimal K is 3.
k3 <- kmeans(DFNumerical, centers = 3, nstart = 25)
# Optimal Visualization
fviz_cluster(k3,data = DFNumerical) 
```
Compare the summary statistics for each cluster and describe each cluster i n this context
(e.g., “Universities with high tuition, l ow acceptance rate...”).

```{r}
# 3 is the Optimal Cluster
k3 <- kmeans(DFNumerical,centers = 3 ,nstart = 25)
#print(k3)
k3$centers # Description of the centers
plot(c(0), xaxt = 'n', ylab = "", type = "l",
     ylim = c(min(k3$centers), max(k3$centers)), xlim = c(0, 18))
# plot centroids
for (i in c(1:3))
  lines(k3$centers[i,], lty = i, lwd = 2) 
# name clusters
text(x = 0.5, y = k3$centers[, 1], labels = paste("Cluster", c(1:3))) # Cluster Names
k3$size # Count of Clusters

#Merging the clusters to the original Data frame
Clusters<-data.frame(k3$cluster)

Clusters<-Clusters%>%rename(clusters=k3.cluster)

UnivAnalysis<-cbind(DFUniver1, Clusters)
head(UnivAnalysis)

ClusterStat<-UnivAnalysis%>%group_by(clusters)%>%summarise(Acceptance_rate=sum(ApplAccept)/sum(ApplRec), AvgOutStateTution=mean(OutStateFee),AvgInStateTution=mean(InStateFee), AvgGradRate=mean(Graduation.rate))

ClusterStat

#Cluster 1 – Universities with highest acceptance rate, Lowest Out State fee but average In state fees but the graduation rate is also low. 

#Cluster 3– Universities with lowest acceptance rate but having highest Out of state fees. But the graduation rate is significantly higher. 

#Cluster 2 – Universities with lowest in state tuition fees and have around 60% graduation rate.

#We can perform all comparision analysis in similar ways. 

```

Using the categorical measurements that were not used i n the analysis (State and
Private/Public) to characterize the different clusters.

```{r}
#State wise values present in the cluster
table(DFUniver1$State, k3$cluster)

#View(UnivAnalysis)
Cluster1 <- UnivAnalysis[UnivAnalysis$clusters == 1,]
#View(Cluster1[,c(1,2,3,21)])

Cluster2 <- UnivAnalysis[UnivAnalysis$clusters == 2,]
#View(Cluster2[,c(1,2,3,21)])

Cluster3 <- UnivAnalysis[UnivAnalysis$clusters == 3,]
#View(Cluster3[,c(1,2,3,21)])

library(ggplot2)
legend <- factor(UnivAnalysis$clusters,levels = c(1,2,3),labels = c("Cluster 1","Cluster 2","Cluster 3"))
ggplot(UnivAnalysis,aes(x=State,y=factor(Pub.Private,levels = c(1,2),labels = c("Public","Private")),
        color=legend))+
         geom_point()+ylab("Public/Private")+xlab("state")+ggtitle("Cluster Plot")

#Plotted the graph which represents all the public and private colleges along with its cluster. Each state has colleges a maximum of 2 out of the 3 clusters. Yes, there is a relationship between clusters and categorical information. 


```
What other external i nformation can explain the contents of some or all of these clusters?
```{r}

k3$withinss # within cluster sum of squares with high ratio as possible
k3$betweenss # mean of distances between cluster centers with ration lower as possible
k3$size # number of points in each cluster
k3$centers # CLuster Centers
#•	Within cluster sum of squares with high ratio as possible
#•	Mean of distances between cluster centers with ration lower as possible
#•	Number of points in each cluster
#•	Cluster Centers
#•	The k value which the highest $withinss is the best choice, because we expect the within sum of squares ratio to be as lower as possible.
```

Consider Tufts University, which is missing some information. Compute the Euclidean
distance of this record from each of the clusters that you found above (using only the
measurements that you have). Which cluster is it closest to? Impute the missing values for
Tufts by taking the average of the cluster on those measurements.
```{r}
library(cluster)

#Finding the Tufts University Data
Tufts<-DFUniver[DFUniver$College.Name =="Tufts University",]
Tufts<-data.frame(Tufts)

#Finding the  NULL fields from the Tufts University   data frame 
Tufts[!complete.cases(Tufts)]

#PTUnderG field has NULL value

#Euclidean Distance
dist(rbind(Tufts, k3$centers[1,]))

dist(rbind(Tufts, k3$centers[2,]))

dist(rbind(Tufts, k3$centers[3,]))

#Cluster 3 has shortest distance 

Avg_PTUnderG <- mean(Cluster3$PTUnderG)

#Update the value 609.22 for the Tuft University 
Tufts[,c(10)]<-Avg_PTUnderG
Tufts[,c(10)]

```




